"use strict";(self.webpackChunkpsdoc_temp=self.webpackChunkpsdoc_temp||[]).push([[110],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return p}});var i=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,i,r=function(e,t){if(null==e)return{};var n,i,r={},a=Object.keys(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=i.createContext({}),c=function(e){var t=i.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=c(e.components);return i.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},f=i.forwardRef((function(e,t){var n=e.components,r=e.mdxType,a=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),f=c(n),p=r,h=f["".concat(s,".").concat(p)]||f[p]||d[p]||a;return n?i.createElement(h,o(o({ref:t},u),{},{components:n})):i.createElement(h,o({ref:t},u))}));function p(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=n.length,o=new Array(a);o[0]=f;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var c=2;c<a;c++)o[c]=n[c];return i.createElement.apply(null,o)}return i.createElement.apply(null,n)}f.displayName="MDXCreateElement"},1635:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return c},toc:function(){return u},default:function(){return f}});var i=n(7462),r=n(3366),a=(n(7294),n(3905)),o=["components"],l={sidebar_position:4},s="Feature reduction",c={unversionedId:"Train-Model/FeatureReduction",id:"Train-Model/FeatureReduction",title:"Feature reduction",description:"Features reduction are done by selecting best features and applying dimensionality reduction techniques. Feature reduction, also known as dimensionality reduction, is the process of reducing the number of features in a resource heavy computation without losing important information. Reducing the number of features means the number of variables is reduced making the computer\u2019s work easier and faster. Feature reduction can be divided into two processes.",source:"@site/docs/Train-Model/FeatureReduction.md",sourceDirName:"Train-Model",slug:"/Train-Model/FeatureReduction",permalink:"/help/docs/Train-Model/FeatureReduction",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Feature generation",permalink:"/help/docs/Train-Model/FeatureGeneration"},next:{title:"Sampling",permalink:"/help/docs/Train-Model/Sampling"}},u=[{value:"Feature selection method",id:"feature-selection-method",children:[{value:"Types of feature selection methods",id:"types-of-feature-selection-methods",children:[{value:"SelectKBest",id:"selectkbest",children:[],level:4},{value:"SelectPercentile",id:"selectpercentile",children:[],level:4},{value:"RFE",id:"rfe",children:[],level:4}],level:3}],level:2},{value:"Scoring Function",id:"scoring-function",children:[{value:"Types of Scoring Function",id:"types-of-scoring-function",children:[{value:"chi2",id:"chi2",children:[],level:4},{value:"f_classif",id:"f_classif",children:[],level:4},{value:"mutual_info_classif",id:"mutual_info_classif",children:[],level:4},{value:"f_regression",id:"f_regression",children:[],level:4},{value:"mutual_info_regression",id:"mutual_info_regression",children:[],level:4}],level:3}],level:2},{value:"Calculate feature score",id:"calculate-feature-score",children:[],level:2},{value:"What is dimensionality reduction?",id:"what-is-dimensionality-reduction",children:[{value:"Dimensionality reduction options",id:"dimensionality-reduction-options",children:[{value:"PCA",id:"pca",children:[],level:4},{value:"Factor Analysis",id:"factor-analysis",children:[],level:4}],level:3}],level:2}],d={toc:u};function f(e){var t=e.components,l=(0,r.Z)(e,o);return(0,a.kt)("wrapper",(0,i.Z)({},d,l,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"feature-reduction"},"Feature reduction"),(0,a.kt)("p",null,"Features reduction are done by selecting best features and applying dimensionality reduction techniques. Feature reduction, also known as dimensionality reduction, is the process of reducing the number of features in a resource heavy computation without losing important information. Reducing the number of features means the number of variables is reduced making the computer\u2019s work easier and faster. Feature reduction can be divided into two processes."),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Feature Selection "),(0,a.kt)("li",{parentName:"ol"},"Feature Extraction")),(0,a.kt)("h2",{id:"feature-selection-method"},"Feature selection method"),(0,a.kt)("p",null," Feature selection is a technique where we choose those features in our data that contribute most to the target variable."),(0,a.kt)("h3",{id:"types-of-feature-selection-methods"},"Types of feature selection methods"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"SelectKBest"),(0,a.kt)("li",{parentName:"ol"},"SelectPercentile"),(0,a.kt)("li",{parentName:"ol"},"RFE ")),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"training.",src:n(3358).Z})),(0,a.kt)("h4",{id:"selectkbest"},"SelectKBest"),(0,a.kt)("p",null,"Select features according to the k highest scores."),(0,a.kt)("h4",{id:"selectpercentile"},"SelectPercentile"),(0,a.kt)("p",null,"Select features according to a percentile of the highest scores."),(0,a.kt)("h4",{id:"rfe"},"RFE"),(0,a.kt)("p",null,"Recursive feature elimination (RFE) is a feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached."),(0,a.kt)("h2",{id:"scoring-function"},"Scoring Function"),(0,a.kt)("p",null,"Based on the selected method score for each feature is calculated."),(0,a.kt)("h3",{id:"types-of-scoring-function"},"Types of Scoring Function"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"chi2"),(0,a.kt)("li",{parentName:"ol"},"f_classif"),(0,a.kt)("li",{parentName:"ol"},"mutual_info_classif"),(0,a.kt)("li",{parentName:"ol"},"f_regression"),(0,a.kt)("li",{parentName:"ol"},"mutual_info_regression")),(0,a.kt)("h4",{id:"chi2"},"chi2"),(0,a.kt)("p",null,"The  chi2(\u03c72)  test is used in statistics to test the independence of two events. More specifically in feature selection we use it to test whether the occurrence of a specific term and the occurrence of a specific class are independent."),(0,a.kt)("h4",{id:"f_classif"},"f_classif"),(0,a.kt)("p",null,"Used only for categorical targets and based on the Analysis of Variance (ANOVA) statistical test."),(0,a.kt)("h4",{id:"mutual_info_classif"},"mutual_info_classif"),(0,a.kt)("p",null,"Used only for categorical targets and based on the Analysis of Variance (ANOVA) statistical test."),(0,a.kt)("h4",{id:"f_regression"},"f_regression"),(0,a.kt)("p",null,"The F value in regression is the result of a test where the null hypothesis is that all of the regression coefficients are equal to zero.  Basically, the f-test compares your model with zero predictor variables (the intercept only model), and decides whether your added coefficients improved the model."),(0,a.kt)("h4",{id:"mutual_info_regression"},"mutual_info_regression"),(0,a.kt)("p",null,"Mutual information between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency."),(0,a.kt)("h2",{id:"calculate-feature-score"},"Calculate feature score"),(0,a.kt)("p",null,"Calculate the relationship between each input variable and the target variable using statistics and selecting those input variables that have the strongest relationship with the target variable."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"training.",src:n(2306).Z})),(0,a.kt)("h2",{id:"what-is-dimensionality-reduction"},"What is dimensionality reduction?"),(0,a.kt)("p",null,"Dimensionality reduction is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables. It can be divided into feature selection and feature extraction."),(0,a.kt)("h3",{id:"dimensionality-reduction-options"},"Dimensionality reduction options"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"PCA"),(0,a.kt)("li",{parentName:"ol"},"Factor Analysis")),(0,a.kt)("h4",{id:"pca"},"PCA"),(0,a.kt)("p",null,"Principal Component Analysis(PCA) is one of  linear dimension reduction. PCA is a projection based method which transforms the data by projecting it onto a set of orthogonal axes."),(0,a.kt)("h4",{id:"factor-analysis"},"Factor Analysis"),(0,a.kt)("p",null,"Factor analysis is a technique that is used to reduce a large number of variables into fewer numbers of factors."))}f.isMDXComponent=!0},2306:function(e,t,n){t.Z=n.p+"assets/images/featurescore-6a628cee741a5b8de00b6ccadad363aa.gif"},3358:function(e,t,n){t.Z=n.p+"assets/images/featureselection-b14bec75a1a773f4cfff7bbb67bfedd4.png"}}]);